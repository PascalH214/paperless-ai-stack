name: paperless-ai-stack
services:
  open-webui:
    image: ghcr.io/open-webui/open-webui:latest
    network_mode: host
    container_name: open-webui
    environment:
      - OLLAMA_BASE_URL=http://localhost:11434
      - ENV=dev
    # ports:
    #   - "2000:8080"
    volumes:
      - ./data/open-webui/data:/app/backend/data

  ollama:
    # Uncomment for amd-gpu
    image: ollama/ollama:rocm
    # Uncomment for nvidia-gpu
    # image: ollama/ollama
    container_name: ollama
    volumes:
      - ./data/ollama/data/:/root/.ollama
      - ./data/ollama/models:/ollama-models
      - ./ollama-entrypoint.sh:/entrypoint.sh
    ports:
      - 11434:11434
    entrypoint: ["/usr/bin/bash", "/entrypoint.sh"]
    environment:
      - OLLAMA_KEEP_ALIVE=-1
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_MODELS=ollama-models
      - OLLAMA_FLASH_ATTENTION=1
      - HSA_OVERRIDE_GFX_VERSION=11.0.0
      - OLLAMA_VRAM_OVERRIDE=19327352832
      - ROCR_VISIBLE_DEVICES=0
    # Uncomment for nvidia-gpu
    # - NVIDIA_DRIVER_CAPABILITIES=all
    # - NVIDIA_VISIBLE_DEVICES=all
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
    # Uncomment for nvidia-gpu

    # Uncomment for amd-gpu
    devices:
      - /dev/kfd
      - /dev/dri
    # Uncomment for amd gpu

  # paperless-ai service
  paperless-ai:
    image: clusterzx/paperless-ai:latest
    container_name: paperless-ai
    depends_on:
      - ollama
    network_mode: host
    # ports:
    #   - "3001:3000"
    environment:
      # paperless-ngx settings
      - PAPERLESS_API_URL=${PAPERLESS_URL}/api
      - PAPERLESS_URL=${PAPERLESS_URL}
      - PAPERLESS_USERNAME=${PAPERLESS_USERNAME}
      - PAPERLESS_API_TOKEN=${PAPERLESS_API_TOKEN}

      # paperless-ai functionality settings
      - RAG_SERVICE_URL=http://localhost:8000
      - RAG_SERVICE_ENABLED=true

      - USE_EXISTING_DATA=yes # avoids that model creates tags/correspondents you dont want -> results in manual creation of this data
      - RESTRICT_TO_EXISTING_TAGS=yes
      - RESTRICT_TO_EXISTING_CORRESPONDENTS=yes
      - RESTRICT_TO_EXISTING_DOCUMENT_TYPES=yes

      - USE_PROMPT_TAGS=no
      - PROMPT_TAGS=

      ## pre-processing needed tags
      - PROCESS_PREDEFINED_DOCUMENTS=yes
      - TAGS=ai-processing-needed

      ## post-processing tags
      - ADD_AI_PROCESSED_TAG=yes
      - AI_PROCESSED_TAG_NAME=ai-processed

      # paperless-ai processing options
      - SCAN_INTERVAL=*/30 * * * *
      - DISABLE_AUTOMATIC_PROCESSING=yes

      # paperless-ai ai settings
      - AI_PROVIDER=ollama
      - OLLAMA_API_URL=http://localhost:11434
      - OLLAMA_MODEL=llama3.2:3b
      - TOKEN_LIMIT=128000
      - RESPONSE_TOKENS=1000

    volumes:
      - ./data/paperless-ai/data:/app/data
  

  # paperless-gpt service
  paperless-gpt:
    image: icereed/paperless-gpt:latest
    container_name: paperless-gpt
    depends_on:
      - ollama
    ports:
      - "4000:8080"
    environment:
      PAPERLESS_BASE_URL: ${PAPERLESS_URL}
      PAPERLESS_API_TOKEN: ${PAPERLESS_API_TOKEN}
      # PAPERLESS_PUBLIC_URL: "http://paperless.mydomain.com" # Optional

      LLM_PROVIDER: "ollama"
      LLM_MODEL: "llama3.2:3b"

      OLLAMA_HOST: "http://ollama:11434"
      OLLAMA_CONTEXT_LENGTH: "32000" # Sets Ollama NumCtx (context window)

      TOKEN_LIMIT: 1000 # Recommended for smaller models

      LLM_LANGUAGE: "English" # Optional, default: English

      OCR_PROVIDER: "llm" # Default OCR provider
      VISION_LLM_PROVIDER: 'ollama' # openai, ollama, mistral, or anthropic
      VISION_LLM_MODEL: "ministral-3:14b" # minicpm-v (ollama) or gpt-4o (openai) or claude-sonnet-4-5 (anthropic/claude)

      AUTO_OCR_TAG: "gpt-ocr-auto"
      AUTO_TAG: "gpt-auto"
      MANUAL_TAG: "gpt-manual"
      PDF_OCR_TAGGING: "true"
      PDF_OCR_COMPLETE_TAG: "gpt-ocr-complete"
      PDF_UPLOAD: "false"

      LOG_LEVEL: "DEBUG" 
    volumes:
      - ./data/paperless-gpt/prompts:/app/prompts # Mount the prompts directory

  dozzle:
    image: amir20/dozzle:latest
    container_name: dozzle
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - 8090:8080
    environment:
      - DOZZLE_ENABLE_ACTIONS=true
      - DOZZLE_ENABLE_SHELL=true
